{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705049d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Splinter\n",
    "# Set an executable path which initializes a browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bdd86",
   "metadata": {},
   "source": [
    "With these two lines of code, we are creating an instance of a Splinter browser. This means that we're prepping our automated browser. We're also specifying that we'll be using Chrome as our browser\n",
    "\n",
    "(****executable_path**) is unpacking the dictionary we've stored the path in â€“ think of it as unpacking a suitcase. \n",
    "\n",
    "**headless=False** means that all of the browser's actions will be displayed in a Chrome window so we can see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fcbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Quotes to Scrape site - This code tells Splinter which site we want to visit by assigning the link to a URL.\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML - use BeautifulSoup to parse the HTML\n",
    "html = browser.html # Creates a html object containing the html from that specified browser\n",
    "html_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4ebe8",
   "metadata": {},
   "source": [
    "### BeautifulSoup parses the HTML text and then stores it as an object\n",
    "\n",
    "### BeautifulSoup has taken a look at the different components and we can now access them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739329ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the Title\n",
    "title = html_soup.find('h2').text\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b087a",
   "metadata": {},
   "source": [
    "We used our html_soup object we created earlier and chained find() to it to **search for the h2 tag**\n",
    "\n",
    "We've also **extracted only the text within the HTML tags by adding .text** to the end of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ba454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the top ten tags\n",
    "# creates a new variable tag_box, which will be used to store the results of a search\n",
    "tag_box = html_soup.find('div', class_='tags-box') #Looking for <div /> elements w/ a class 'tags-box'\n",
    "\n",
    "# tag_box\n",
    "    # This variable will hold the results of a find_all, but this time we're searching through the \n",
    "    # parsed results stored in our tag_box variable to find <a /> elements(tags) with a 'tag' class\n",
    "tags = tag_box.find_all('a', class_='tag')\n",
    "\n",
    "# Cycles through each tag in the tags variable, strips the HTML code out of it, \n",
    "# and then prints only the text of each tag. \n",
    "for tag in tags:\n",
    "    word = tag.text\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already created these variables - but this is an example if using another website\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Look to visit the first 5 pages of the website\n",
    "for x in range(1, 6):\n",
    "    # Parsing the html using BeautifulSoup\n",
    "    html = browser.html \n",
    "    quote_soup = soup(html, 'html.parser')\n",
    "    \n",
    "    # Creating a variable using soup to find/hold all the tags with the span (tags) w 'text' class/atr\n",
    "    quotes = quote_soup.find_all('span', class_='text')\n",
    "    \n",
    "    # For each quote in our variable holding all tags, we will print the page its from and the texts\n",
    "    for quote in quotes:\n",
    "        print('page:', x, '----------')\n",
    "        print(quote.text)\n",
    "    \n",
    "    # Clicks the next button using splinter to move on to the next page\n",
    "    browser.links.find_by_partial_text('Next').click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
